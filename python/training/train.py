import os
import numpy as np
from stable_baselines3 import PPO
from imitation.algorithms.bc import BC
from stable_baselines3.common.vec_env import DummyVecEnv
from imitation.data.types import Transitions

from training.azul_env import AzulEnv
from baseGame.assets import *
from bots import greedy_agent


def Train_Game(model_name, learner_index=0, total_timesteps=10_000_000, demo_path=None):
    """
    Trains a PPO agent for a specific player index in AzulEnv.

    Parameters:
        model_name: str - name to save/load the model.
        learner_index: int - which player index (0-3) is being trained.
        total_timesteps: int - PPO training steps.
        demo_path: str - optional path for demo file (autogenerated if None).
    """

    if demo_path is None:
        demo_path = f"models/greedy_agent_p{learner_index}.npz"
    model_path = f"models/{model_name}"

    # Define agent roles
    agent_types = ["greedy"] * 4
    agent_types[learner_index] = "learn"

    # Create environment
    env = DummyVecEnv([lambda: AzulEnv(agent_types=agent_types)])
    env_instance = env.envs[0]

    # Step 1: Generate demonstrations if needed
    if not os.path.exists(demo_path):
        print(f"Generating greedy demonstrations for player {learner_index}...")
        obs_list, acts_list = [], []

        for _ in range(1000):
            obs, _ = env.reset()
            done = False

            while not done:
                player_index = env_instance.turn_index % 4
                if player_index != learner_index:
                    # Let non-learning agents play automatically
                    obs, _, done, _, _ = env.step(0)
                    continue

                player = env_instance.players[learner_index]
                move = greedy_agent(env_instance.game, player, learner_index)
                action = ACTIONS.index(move) if move in ACTIONS else 0

                obs_list.append(obs[0])
                acts_list.append(action)

                obs, _, done, _, _ = env.step([action])

        np.savez(demo_path, obs=np.array(obs_list), acts=np.array(acts_list))
        print("Saved demonstrations to", demo_path)
    else:
        print("Loading cached demonstrations from", demo_path)

    # Step 2: Initialize or load model
    policy_kwargs = dict(net_arch=[64, 64])

    if os.path.exists(f"{model_path}.zip"):
        print(f"Loading existing model from {model_path}...")
        model = PPO.load(model_path, env=env)
    else:
        print("Pretraining from behavior cloning...")
        data = np.load(demo_path)
        transitions = Transitions(
            obs=data["obs"],
            acts=data["acts"],
            dones=np.zeros(len(data["obs"]), dtype=bool),
            infos=[{}] * len(data["obs"]),
            next_obs=data["obs"]
        )

        # Dummy PPO to provide a policy for BC
        temp_model = PPO("MlpPolicy", env, policy_kwargs=policy_kwargs, verbose=0)
        policy = temp_model.policy

        bc_trainer = BC(
            observation_space=env.observation_space,
            action_space=env.action_space,
            demonstrations=transitions,
            rng=np.random.default_rng(0),
            policy=policy,
        )
        bc_trainer.train(n_epochs=20)
        print("Behavior cloning complete.")

        # Inject into PPO
        model = PPO("MlpPolicy", env, verbose=1, policy_kwargs=policy_kwargs)
        model.policy.load_state_dict(bc_trainer.policy.state_dict())

    # Step 3: PPO fine-tuning
    print("Training with PPO...")
    model.learn(total_timesteps=total_timesteps)
    model.save(model_path)
    print(f"Model saved to {model_path}")
